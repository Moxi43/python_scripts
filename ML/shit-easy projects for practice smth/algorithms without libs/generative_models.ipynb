{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597000144069",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk #helper for data cleaning\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         Date  ArticleCode Language  \\\n0  2015-01-05            1       en   \n1  2015-01-05            2       fr   \n2  2015-01-05            3       en   \n3  2015-01-05            4       en   \n4  2015-01-05            5       en   \n5  2015-01-05            6       en   \n6  2015-01-05            7       en   \n7  2015-01-05            8       en   \n8  2015-01-05            9       en   \n9  2015-01-05           10       en   \n\n                                               Title  \\\n0  WTO celebrates 20 years of helping global econ...   \n1  L'Organisation mondiale du commerce l'preuve d...   \n2  Delegates of 45 nations,WTO Chief to be at CII...   \n3  Compliance Rulings On Aircraft, Tuna, COOL To ...   \n4  Sudan making new push at WTO membership: official   \n5       China Ends Rare-Earth Minerals Export Quotas   \n6  China frees prices of commodities, services in...   \n7  India bans imports on account of avian influen...   \n8  Bdesh battles to get backing from peers as tim...   \n9  Different tariff-cut goals drag 16 Asia-Pacifi...   \n\n                                             Content  \n0  GENEVA, Jan 1 (KUNA) -- The World Trade Organi...  \n1  5 January 2015 Les Echos L'OMC s'est donn six ...  \n2  New Delhi, Jan 2 (PTI) Over 1,000 delegates fr...  \n3  World Trade Online Posted: December 30, 2014 A...  \n4  Sudan Tribune 4 January 2015 January 3, 2015 (...  \n5  China Ends Decade-Old Quota System Limiting Ex...  \n6  SHANGHAI, Jan 5 (Reuters) - China has freed pr...  \n7  Dilasha Seth & Madhvi Sally, ET Bureau Economi...  \n8  Shamsul Huda 4 January 2015 The Financial Expr...  \n9  Kyodo News TOKYO, Jan. 2 -- Negotiations of a ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>ArticleCode</th>\n      <th>Language</th>\n      <th>Title</th>\n      <th>Content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015-01-05</td>\n      <td>1</td>\n      <td>en</td>\n      <td>WTO celebrates 20 years of helping global econ...</td>\n      <td>GENEVA, Jan 1 (KUNA) -- The World Trade Organi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2015-01-05</td>\n      <td>2</td>\n      <td>fr</td>\n      <td>L'Organisation mondiale du commerce l'preuve d...</td>\n      <td>5 January 2015 Les Echos L'OMC s'est donn six ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2015-01-05</td>\n      <td>3</td>\n      <td>en</td>\n      <td>Delegates of 45 nations,WTO Chief to be at CII...</td>\n      <td>New Delhi, Jan 2 (PTI) Over 1,000 delegates fr...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2015-01-05</td>\n      <td>4</td>\n      <td>en</td>\n      <td>Compliance Rulings On Aircraft, Tuna, COOL To ...</td>\n      <td>World Trade Online Posted: December 30, 2014 A...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2015-01-05</td>\n      <td>5</td>\n      <td>en</td>\n      <td>Sudan making new push at WTO membership: official</td>\n      <td>Sudan Tribune 4 January 2015 January 3, 2015 (...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2015-01-05</td>\n      <td>6</td>\n      <td>en</td>\n      <td>China Ends Rare-Earth Minerals Export Quotas</td>\n      <td>China Ends Decade-Old Quota System Limiting Ex...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2015-01-05</td>\n      <td>7</td>\n      <td>en</td>\n      <td>China frees prices of commodities, services in...</td>\n      <td>SHANGHAI, Jan 5 (Reuters) - China has freed pr...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2015-01-05</td>\n      <td>8</td>\n      <td>en</td>\n      <td>India bans imports on account of avian influen...</td>\n      <td>Dilasha Seth &amp; Madhvi Sally, ET Bureau Economi...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2015-01-05</td>\n      <td>9</td>\n      <td>en</td>\n      <td>Bdesh battles to get backing from peers as tim...</td>\n      <td>Shamsul Huda 4 January 2015 The Financial Expr...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2015-01-05</td>\n      <td>10</td>\n      <td>en</td>\n      <td>Different tariff-cut goals drag 16 Asia-Pacifi...</td>\n      <td>Kyodo News TOKYO, Jan. 2 -- Negotiations of a ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df = pd.read_pickle('C:\\\\code\\\\ML-practicE\\\\shit-easy projects for practice smth\\\\algorithms without libs\\\\datasets\\\\df_save.pck')\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['In',\n 'a',\n 'little',\n 'or',\n 'a',\n 'european_union',\n 'little',\n 'bit',\n 'world_trade_organization']"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#tokenization test\n",
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer([('world', 'bank'), ('world', 'trade', 'organization'), ('doha', 'round'),\n",
    "                          ('united', 'states'), ('european', 'union'), ('new', 'zealand'),\n",
    "                          ('per', 'cent'),('south', 'korea'),\n",
    "                          ])\n",
    "\n",
    "# Test the tokenizer\n",
    "tokenizer.tokenize('In a little or a european union little bit world trade organization'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Максим\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "#nltk's english stopwords as var called 'stopwords'\n",
    "nltk.download('stopwords')\n",
    "my_stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [words for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-da3621de8e6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtotalvocab_stemmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtotalvocab_tokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mallwords_stemmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_and_stem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtotalvocab_stemmed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallwords_stemmed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "#perform tokenization and stemming \n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in texts:\n",
    "    allwords_stemmed = tokenize_and_stem(i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "\n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'totalvocab_tokenized' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4154f89f13c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#updated df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvocab_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtotalvocab_tokenized\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotalvocab_stemmed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'there are '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' items in vocab_frame'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'totalvocab_tokenized' is not defined"
     ]
    }
   ],
   "source": [
    "#updated df\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}