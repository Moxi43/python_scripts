{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#data manipulations\n",
    "import pandas as pd\n",
    "#matrix data structures\n",
    "from patsy import dmatrices \n",
    "#for error logging\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs probability between 0 and 1, used to help define our logistic regression curve \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) #set the seed\n",
    "#Define model params\n",
    "tol=1e-8 \n",
    "lam = None\n",
    "#how long to train it\n",
    "max_iter = 20\n",
    "## data creation settings\n",
    "#Covariance measures how two variables move together. \n",
    "#It measures whether the two move in the same direction (a positive covariance) \n",
    "#or in opposite directions (a negative covariance).## data creation settings\n",
    "#Covariance measures how two variables move together. \n",
    "#It measures whether the two move in the same direction (a positive covariance) \n",
    "#or in opposite directions (a negative covariance).\n",
    "r = 0.95 #covariance between x and z\n",
    "n = 1000 #number of observations (size of dataset to generate)\n",
    "sigma = 1 #variance of noise - how spread out is the data\n",
    "\n",
    "##model setting\n",
    "beta_x, beta_z, beta_v = -4, .9, 1 #true beta coefficients\n",
    "var_x, var_z, var_v = 1, 1, 4 #variances of inputs\n",
    "\n",
    "##the model specification i want to fit\n",
    "formula = 'y ~ x + z + v + np.exp(x) + I(v**2 + z)'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    Intercept         x         z           v  np.exp(x)  I(v ** 2 + z)\n0         1.0 -0.897148 -0.314119  545.367536   0.407731  297425.435732\n1         1.0 -0.447369 -0.087993  -49.748521   0.639308    2474.827344\n2         1.0  1.604657  1.828273    0.096501   4.976154       1.837585\n3         1.0 -0.150961  0.090954   -0.166098   0.859881       0.118542\n4         1.0 -0.500006 -1.213495  162.959910   0.606527   26554.718908\n..        ...       ...       ...         ...        ...            ...\n95        1.0  0.927424  1.045993  -59.877718   2.527988    3586.387103\n96        1.0  0.525556  0.865407   19.942727   1.691398     398.577754\n97        1.0  0.376862  0.861954   -0.021452   1.457703       0.862414\n98        1.0 -0.453325 -0.371617  -75.657133   0.635512    5723.630109\n99        1.0  1.083920  1.187995   -0.176072   2.956244       1.218996\n\n[100 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Intercept</th>\n      <th>x</th>\n      <th>z</th>\n      <th>v</th>\n      <th>np.exp(x)</th>\n      <th>I(v ** 2 + z)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>-0.897148</td>\n      <td>-0.314119</td>\n      <td>545.367536</td>\n      <td>0.407731</td>\n      <td>297425.435732</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>-0.447369</td>\n      <td>-0.087993</td>\n      <td>-49.748521</td>\n      <td>0.639308</td>\n      <td>2474.827344</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>1.604657</td>\n      <td>1.828273</td>\n      <td>0.096501</td>\n      <td>4.976154</td>\n      <td>1.837585</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.150961</td>\n      <td>0.090954</td>\n      <td>-0.166098</td>\n      <td>0.859881</td>\n      <td>0.118542</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>-0.500006</td>\n      <td>-1.213495</td>\n      <td>162.959910</td>\n      <td>0.606527</td>\n      <td>26554.718908</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>1.0</td>\n      <td>0.927424</td>\n      <td>1.045993</td>\n      <td>-59.877718</td>\n      <td>2.527988</td>\n      <td>3586.387103</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>1.0</td>\n      <td>0.525556</td>\n      <td>0.865407</td>\n      <td>19.942727</td>\n      <td>1.691398</td>\n      <td>398.577754</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>1.0</td>\n      <td>0.376862</td>\n      <td>0.861954</td>\n      <td>-0.021452</td>\n      <td>1.457703</td>\n      <td>0.862414</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>1.0</td>\n      <td>-0.453325</td>\n      <td>-0.371617</td>\n      <td>-75.657133</td>\n      <td>0.635512</td>\n      <td>5723.630109</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>1.0</td>\n      <td>1.083920</td>\n      <td>1.187995</td>\n      <td>-0.176072</td>\n      <td>2.956244</td>\n      <td>1.218996</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "#Step 2 - Generate and organize data\n",
    "x, z  = np.random.multivariate_normal([0, 0], [[var_x,r], [r, var_z]], n).T\n",
    "#blood pressure\n",
    "v = np.random.normal(0, var_v, n)**3\n",
    "\n",
    "#create a pandas dataframe\n",
    "A = pd.DataFrame({'x': x, 'z': z, 'v': v})\n",
    "\n",
    "#compute the log(odds) for 3 independent vars\n",
    "#using the sigmoid func\n",
    "A['log_odds'] = sigmoid(A[['x', 'z', 'v']].dot([beta_x, beta_z, beta_v]) +                  sigma*np.random.normal(0, 1, n))\n",
    "#compute the probability sample from polynomial distribution\n",
    "#A binomial random var is the number of successes x has in n repeated trials of binomial #experiment.\n",
    "#The probablity distribution of a binomial random var is called a binomial distribution.\n",
    "A['y'] = [np.random.binomial(1, p) for p in A.log_odds]\n",
    "\n",
    "#create a dataframe that encompasses input data, model formula, and outputs\n",
    "y, X = dmatrices(formula, A, return_type='dataframe')\n",
    "\n",
    "#print it\n",
    "X.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#like dividing by zero \n",
    "def catch_singularity(f):\n",
    "    '''Silences LinAlg Errors and throws a warning instead'''\n",
    "\n",
    "    def silencer(**args, **kwargs):\n",
    "        try:\n",
    "            return f(**args, **kwargs)\n",
    "        except np.linalg.LinAlgError:\n",
    "            warnings.warn('Algorithm terminated - singular Hessian!')\n",
    "            return args[0]\n",
    "    return silencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@catch_singularity\n",
    "def newton_step(curr, X, lam=None):\n",
    "    '''One naive step of Newton's Method'''\n",
    "    \n",
    "    #how to compute inverse? http://www.mathwarehouse.com/algebra/matrix/images/square-matrix/inverse-matrix.gif\n",
    "    \n",
    "    ## compute necessary objects\n",
    "    #create probability matrix, miniminum 2 dimensions, tranpose (flip it)\n",
    "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T\n",
    "    #create weight matrix from it\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    #derive the hessian \n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    #derive the gradient\n",
    "    grad = X.T.dot(y-p)\n",
    "    \n",
    "    ## regularization step (avoiding overfitting)\n",
    "    if lam:\n",
    "        # Return the least-squares solution to a linear matrix equation\n",
    "        step, *_ = np.linalg.lstsq(hessian + lam*np.eye(curr.shape[0]), grad)\n",
    "    else:\n",
    "        step, *_ = np.linalg.lstsq(hessian, grad)\n",
    "        \n",
    "    ## update our \n",
    "    beta = curr + step\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@catch_singularity\n",
    "def alt_newton_step(curr, X, lam=None):\n",
    "    '''One naive step of Newton's Method'''\n",
    "    \n",
    "    ## compute necessary objects\n",
    "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    grad = X.T.dot(y-p)\n",
    "    \n",
    "    ## regularization\n",
    "    if lam:\n",
    "        #Compute the inverse of a matrix.\n",
    "        step = np.dot(np.linalg.inv(hessian + lam*np.eye(curr.shape[0])), grad)\n",
    "    else:\n",
    "        step = np.dot(np.linalg.inv(hessian), grad)\n",
    "        \n",
    "    ## update our weights\n",
    "    beta = curr + step\n",
    "    \n",
    "    return beta@catch_singularity\n",
    "def alt_newton_step(curr, X, lam=None):\n",
    "    '''One naive step of Newton's Method'''\n",
    "    \n",
    "    ## compute necessary objects\n",
    "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    grad = X.T.dot(y-p)\n",
    "    \n",
    "    ## regularization\n",
    "    if lam:\n",
    "        #Compute the inverse of a matrix.\n",
    "        step = np.dot(np.linalg.inv(hessian + lam*np.eye(curr.shape[0])), grad)\n",
    "    else:\n",
    "        step = np.dot(np.linalg.inv(hessian), grad)\n",
    "        \n",
    "    ## update our weights\n",
    "    beta = curr + step\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coefs_convergence(beta_old, beta_new, tol, iters):\n",
    "    '''Checks whether the coefficients have converged in the l-infinity norm.\n",
    "    Returns True if they have converged, False otherwise.'''\n",
    "    #calculate the change in the coefficients\n",
    "    coef_change = np.abs(beta_old - beta_new)\n",
    "    \n",
    "    #if change hasn't reached the threshold and we have more iterations to go, keep training\n",
    "    return not (np.any(coef_change>tol) & (iters < max_iter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initial conditions\n",
    "#initial coefficients (weight values), 2 copies, we'll update one\n",
    "beta_old, beta = np.ones((len(X.columns),1)), np.zeros((len(X.columns),1))\n",
    "\n",
    "#num iterations we've done so far\n",
    "iter_count = 0\n",
    "#have we reached convergence?\n",
    "coefs_converged = False\n",
    "\n",
    "#if we haven't reached convergence... (training step)\n",
    "while not coefs_converged:\n",
    "    \n",
    "    #set the old coefficients to our current\n",
    "    beta_old = beta\n",
    "    #perform a single step of newton's optimization on our data, set our updated beta values\n",
    "    beta = newton_step(beta, X, lam=lam)\n",
    "    #increment the number of iterations\n",
    "    iter_count += 1\n",
    "    \n",
    "    #check for convergence between our old and new beta values\n",
    "    coefs_converged = check_coefs_convergence(beta_old, beta, tol, iter_count)\n",
    "    \n",
    "print('Iterations : {}'.format(iter_count))\n",
    "print('Beta : {}'.format(beta))\n",
    "\n",
    "\n",
    "\n",
    "## initial conditions\n",
    "#initial coefficients (weight values), 2 copies, we'll update one\n",
    "beta_old, beta = np.ones((len(X.columns),1)), np.zeros((len(X.columns),1))\n",
    "\n",
    "#num iterations we've done so far\n",
    "iter_count = 0\n",
    "#have we reached convergence?\n",
    "coefs_converged = False\n",
    "\n",
    "#if we haven't reached convergence... (training step)\n",
    "while not coefs_converged:\n",
    "    \n",
    "    #set the old coefficients to our current\n",
    "    beta_old = beta\n",
    "    #perform a single step of newton's optimization on our data, set our updated beta values\n",
    "    beta = newton_step(beta, X, lam=lam)\n",
    "    #increment the number of iterations\n",
    "    iter_count += 1\n",
    "    \n",
    "    #check for convergence between our old and new beta values\n",
    "    coefs_converged = check_coefs_convergence(beta_old, beta, tol, iter_count)\n",
    "    \n",
    "print('Iterations : {}'.format(iter_count))\n",
    "print('Beta : {}'.format(beta))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594676301672",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}